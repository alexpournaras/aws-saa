Amazon S3

advertised as 'infinitelly scaling storage'

use cases:
backup and storage
disaster recovery
archive
hybris cloud storage
application hosting
media hosting (video, images, files)
data lakes & big data analytics
software delivery
static website

s3 buckets

* store objects (files) in buckets (directories/folders)
* buckets must have a globally unique name (accross all regions and all users)
* buckets are defind in region level!! (important)
* naming convention: 
    - no uppercase, no underscore
    - 3-63 characters long
    - not an ip
    - must start with lowecase letter or number
    - must NOT start with the prefix xn--
    - must Not end with the suffix -s3alias

s3 objects

objects (files) have a key. the key is the FULL PATH

s3://my-bucket/[my_file.txt]
s3://my-bucket/[my_folder/another_folder/my_file.txt]

key is composed of prefix: my_folder/another_folder/
and the object name: my_file.txt

there is no concept of directiories!!! the ui seems like it but its not!!! (important)

object values are the content:
- max object size is 5TB
- if uploading more than 5GB, must use `multi-part upload`

they can have metadata, tags and version id if versioning is enabled

when objects are not for public, their public url will not work, unless we press the open button that will open the url but with a pre signed signature that will basically have our credentials encrypted, therefore it will allow us to see the file.

--

security

* user based: iam policies -> which is api calls should be allowed for a specific user from iam

* resource-based:
    - bucket policeis => bucket wide rules from the s3 console - allows cross account
    - object access control list (acl) -> finer grain (can be disabled)
    - bucket access control list (acl) -> less common (can be disabled)

* an iam pricipal can access s3 object if:
the user iam permissions ALLOW it OR the resource policy ALLOWS its, AND there no explicit DENY.

* encryption

s3 bucket policies:

json documents
resources: buckets and objects
effect: allow/deny
actions: set of api calls to allo or deny
principal: the account or user to apply policy to

use s3 bucket for policy to:
* grant public access to the bucket
* froce objects to be encrypted at upload
* grant access to another account (cross-account)

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "PublicRead",
      "Effect": "Allow",
      "Principal": "*",
      "Action": [
        "s3:GetObject",
      ],
      "Resource": ["arn:aws:s3:::mybucket/*"]
    }
  ]
}
```

Allow (effect) everyone (principal) to see objects (action) from the s3 bucket 'mybucket' (resource)

* when a website user needs to have access to an object, we create a bucket policy to allow public access
* when an iam user needs to have access to an object, we attack an iam policy that will allow user to see this s3 object
* when ec2 needs to have access to an s3 object, we attach an iam instance Role that have the corresponding iam permissions
* when there is another iam user from another aws account and we want to give access, we create a bucket policy that allows cross-account

bucket settings for block public access
* there is a list of checkboxes that are enabled to block public access
* these settings were created to prevent company data leaks
* if we want the data in our bucket to stay private, leave these checkboxes on
* can be set at the account level.

--

versioning

* is enabled at bucket level
* same key overwrite will change the version: 1, 2, 3...
* its best practive to version the buckets
    - protect agains unintened deletes
    - ability to restore previous version (roll back)

* any file that is not versioned will have version 'null'.
* suspeneding versioning does not delete the previous versions

* to roleback, we delete the latest version.
* if we delete a file with no previous versions, it will create a delete marker. to bring it back, we delete the delete marker.

--
replication

* must enable versioning in source and destination
* cross-region replication (CRR)
* same-region replication (SRR)
* buckets can be in different AWS accounts
* copying is ASNYCRONOUS
* must give proper iam permissions to s3

use cases:
crr: compiance, lower latency access, replication accros accounts
srr: log aggregation, live replication between production and test accounts (for test environments)

* after we enable replication, only new objects are replicated
* optionally we can replicate existing objects using s3 batch replication
* for DELETE operations: we can replicate delete markers from source to target (optional setting), deletions with a version id are not replicated to avoid malicious deletes!!!! (important) (replication rule -> delete marker replication)
* there is no chaining of replications. if A replicates to B and B to C, then A will NOT replicate to C. to do multi replication we need rules from A to B and from A to C

--

s3 storage classes

* durability: 
  - high durability 99.9999999% (11 9's) of objects accros multiple AZ
  - for 10.000.000 objects, we could lose 1 object every 10.000 years
  - same for all storage classes

* availability:
  - measures how readily available a service is
  - varies depnding on storage class
  - 99.99% availability = not available 53 minutes a year

* we can automate moving objects between storage classes with lifecycle rules in the buckets.
* example: go to standard-IA after 30 days 

## standard - general purpose
* 99.99% availability
* used for frequently accessed data
* low latency and high throughput
* sustain 2 concurrent facility failures (on aws)
* use cases: big data analytics, mobile & gaming applications, content distribution

## infrequent access
* for less frequntly accessed data but rapid access when needed
* lower cost than s3 standard, but it has cost on retrieval.

* s3-standard-infrequent access (standart-IA)
  - 99.9% availability
  - use cases: disaster recovery, backups

* one zone-infrequent access (one zone-IA)
  - high durability in a single az
  - 99.5% availability
  - use cases: storing secondary backup compies of on-premise data, or data we can recreate = recreate important!!!!!

## glacier storag eclasses
* low cost objecet storage meant for achiving / backup
* pricing: price for storage + object retrieval cost

* glacier instant retrieval
  - miliseconds retrieval, great for data accessed once a quarter
  - minimum storage duration of 90 days

* glacier flexible retrieval
  - expedited (1-5 minutes), stardard (3-5 hours), bulk => free!! (5-12 hours)
  - minimum storage duration of 90 days

* glacier deep archive - for long term storage
  - standard (12 hours), bulk (48 hours)
  - minimum storage duration of 180 days
  - lowest cost of all

## s3 intelligent tiering
* small monthly monitoring and auto-tiering fee
* moves objects automatically between access tiers based on usage
* there are no retrieval charges!!! (important)

## s3 express one zone
* high performance, single az 
* objects stored in a `directory bucket`
* handle 100.00s requests per second with <10 milisecond latency
* up to 10x better performance thant s3 standard (50% lower costs)
* availability 99.95%
* this is powerfull and fast because its in the same az and that reduces latency.
* use cases: latency-sensitive apps, data intensive apps, ai & ml training, fincancial modeling, media processing, hpc
* best integrated with sagemaker model training, athena, emr, glue

--

moving between storage classes
* from standard to IA to Glacier to Glacier Deep Archive
* moving objects can be automated using `Lifecycle Rules`

lifecycle rules
* transition actions:
  - move objects to IA 60 days after creating
  - move to glacier for archiving after 6 months

* expiration actions:
  - access log files can be set to delete after 365 days
  - can be used to delete old versions of files (if versioning is enabled)
  - can be used to delete incomplete multi part uploads

* rules can be created for certain prefix (s3://mybucket/mp3/*)
* rules can be created for certain tags (department: finance)

amazon s3 analytics
* help you decide when to transition objects to the right storage class
* recommendations for standard and standard IA (does not work for one zone ia or glacier)
* report is updated daily and its a csv file
* 24 to 48 hours to start seeing data nalysis
* good first step to put together lifecycle rules or improve them.

--

s3 - requester pays
* by default owner pays for storage cost and networking cost (downloads).
* but with requester pays buckets, the owner only pays for the storage costs.
* the networking is billed to the requester.
* helpful when sharing large datasets with other accounts
* requester must be in AWS so they know who to bill (cannot be anonymous)

--

s3 event notifications
* S3:ObjectCreated, S3:ObjectRemoved etc etc.
* can be filtered (*.jpg)
* use case: generate thumbnails of images uploaded to s3
* can create as many s3 events as desired
* typically deliver events in targets (SNS, SQS, Lambda functions) in seconds but can take a minute or longer

iam permissions
targets should have the corresponding policy!!! (similar to bucket policy)

another target is amazon eventbridge where ALL events are going there no matter what. Then with rules can be passed in other aws services
* advanced filtering with json rules (metadata, object size, name)
* multiple desinations: step functions, kinesis streams, firehose
* eventbridge capabilityies: archive, replay events, reliable delivery

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "SqsAccessPolicy999",
      "Effect": "Allow",
      "Principal": "*",
      "Action": "sqs:SendMessage",
      "Resource": "arn:aws:sqs:eu-west-1:21321321:DemoS3Notification"
    }
  ]
}
```

--

s3 baseline performance
* scales to high requests rates, latency 100-200 ms
* 3500 PUT/COPY/POST/DELETE or 5500 GET/HEAD requests PER SECOND, PER PREFIX!!!!! 
* no limit on number of prefixes in a bucket
  - bucket/[folder1]/file1
  - bucket/[folder2]/file2
* we can spread reads across all prefixes evenly to achive higher number of requests per second!!!!! (important)

multipart upload:
* recommended for files > 100mb
* must use for files > 5gb
* big file -> divide in parts -> parallel uploads (speed up transfers) -> s3

s3 transfer acceleration
* increase transfer speed by transfering first to aws edge location and then s3 bucket in the target region
* compatible with multi part upload
* big file (usa) -> public internet -> aws edge location usa -> private internet aws -> s3 bucket in astralia

byte range fetches
* parallelize GETs for faster downloads
* request different parts of a file
* can be used to retrieve only partial data (example only the header that has some metadata explaing the rest of the file, first 100 bytes for example)
* better resilience in case of failures

--

s3 batch operations
* perform bulk operations on existing s3 objects with a single request
  - modify objects metadata & properties
  - copy objects between s3 buckets
  - encrypt unencrypted objects
  - modify ACLs, tags
  - restore objects from s3 glacier
  - invoke lambda function to perform custom actions on each object

* A job consists of:
  - a list of objects
  - the action to perform
  - optional parameters

s3 batch operations are helpful (in comparison with custom scripts) because
* mange retries, track progress, send completion notifications, generate reports

* we can use S3 Inventory to generate a list of objects, pass it to Athena to filter them, and give to s3 batch operations to do the job.

